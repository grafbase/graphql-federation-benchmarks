# Coding guidelines

Code MUST be clear and concise and the same applies to comments. Clear code does not need comments explaining what it does. But you may explain _why_ you're solving a problem in a particular way.

Prefer re-using existing files. For new components, avoid creating many small files.

## Rust

### Error handling

In production code, always handle `Result` and `Option` with care when you need to retrieve the value:

```rust
# propagate errors with `?`
let result = f()?;

# custom handling
match f() {
    Ok(v) => g(v),
    Err(e) => h(e)
}
```

If it can never be `None` or an error, use `expect("reason")` or `unreachable!("reason")` in a match case rather than `.unwrap()`.

### String formatting

Prefer Use modern string formatting `format!("{value}")` rather than `format!("{}", value)`.

### Logging

Do not abuse of the `info` level with logging such as `tracing::info!`, use `debug` instead.

### Lints

You may check for errors with:

```sh
cargo clippy -p <package>
```

### Tests

In test code, you may use `.unwrap()`.

Avoid test names like `test_` and prefer using terms like "can", "should", "must" or "given" to define a property/behavior the code should uphold.

Use inline snapshots with `insta` crate rather than a chain of `assert_eq!`:

```rust
insta::assert_json_snapshot!(value, @r#""#);
// or
insta::assert_snapshot!(error, @r#""#);
```

If you do want to compare to variables, import the `assert_*` variants from the `pretty_asesertions` crate.

To generate the initial snapshots, you may run the tests with the `INSTA_FORCE_PASS=1` environment variable and use `cargo insta approve` to apply all the snapshots.

You can run tests with `nextest` as follows:

```sh
cargo nextest run -p <package>
# or for a particular set of tests
cargo nextest run -p <package> <test prefix or name>
```

## Markdown conventions

- Always use an empty line after a heading and before starting a list.

# Structure

## Project Overview

This is a GraphQL Federation Benchmarking System that orchestrates performance tests across different federation gateways. It runs K6 load tests while monitoring gateway resource usage (CPU/memory) and produces detailed JSON reports.

## Architecture

```
graphql-federation-benchmarks/
├── crates/
│   ├── cli/              # Main CLI application
│   │   └── src/
│   │       ├── main.rs           # CLI entry point with tracing setup
│   │       ├── cli/              # Command definitions using argh
│   │       │   ├── mod.rs        # CLI context and command enum
│   │       │   ├── run.rs        # Run benchmarks command
│   │       │   └── list.rs       # List configurations command
│   │       ├── benchmark.rs      # Benchmark configuration loading and execution
│   │       ├── docker.rs         # Docker operations (compose, containers)
│   │       ├── gateway.rs        # Gateway management and configuration
│   │       ├── resources.rs      # Resource monitoring via Docker stats API
│   │       ├── k6.rs             # K6 test execution and result parsing
│   │       ├── report.rs         # Output data structures and report generation
│   │       └── system.rs         # System information detection
│   └── subgraphs/        # Subgraph implementations
│       ├── big-response/
│       └── many-plans/
│
├── benchmarks/           # Benchmark configuration files
│   └── [name].toml       # Defines matrix of scenarios × gateways to run
│
├── scenarios/            # K6 test scenarios
│   ├── config.toml       # Maps scenario names to supergraphs
│   └── [scenario-name]/
│       ├── k6.js         # K6 load test script
│       └── body.json     # Request body for the test
│
├── supergraphs/          # Supergraph schemas and configurations
│   ├── config.toml       # Maps supergraphs to required subgraphs
│   └── [supergraph-name]/
│       ├── schema.graphql        # Federation supergraph schema
│       └── cosmo/                # Vendor-specific configs
│           └── supergraph.json
│
├── gateways/             # Gateway configurations
│   ├── config.toml       # Gateway definitions (image, args, env)
│   └── [gateway-name]/   # Gateway-specific config files
│       └── [config files]
│
└── compose.yml           # Main docker compose with all subgraph services
```

## How It Works

The benchmark system follows a layered configuration approach:

1. **Benchmark Configuration** (`benchmarks/[name].toml`): Defines a matrix of scenarios and gateways to run
   ```toml
   [[benchmarks]]
   scenario = "big-response"
   gateway = ["grafbase", "cosmo", "apollo-router"]
   ```

2. **Scenario Configuration** (`scenarios/config.toml`): Maps scenarios to supergraphs
   ```toml
   [big-response]
   supergraph = "big-response"
   ```

3. **Supergraph Configuration** (`supergraphs/config.toml`): Specifies which subgraphs to activate
   ```toml
   [big-response]
   subgraphs = ["big-response"]
   ```

4. **Execution Flow**:
   - CLI accepts optional benchmark config name: `cargo run -- run [config-name]`
   - Loads the benchmark matrix from `benchmarks/[config-name].toml`
   - For each scenario × gateway combination:
     - Resolves scenario → supergraph → subgraphs chain
     - Starts only required subgraph services from main `compose.yml`
     - Starts gateway container with `/supergraph` volume mount
     - Runs K6 test from `scenarios/[scenario-name]/k6.js`
     - Collects resource metrics during execution
     - Generates performance report

## Key Components

### Benchmark Manager (`crates/cli/src/benchmark.rs`)

- Loads benchmark configurations from `benchmarks/[name].toml`
- Resolves scenario → supergraph → subgraphs mapping chain
- Orchestrates benchmark execution workflow:
  1. Start specific subgraphs via docker compose
  2. Start gateway container with supergraph mount
  3. Health check gateway
  4. Run K6 test with resource monitoring
  5. Generate results
  6. Cleanup resources

### Gateway Manager (`crates/cli/src/gateway.rs`)

- Loads gateway configurations from `gateways/config.toml`
- Manages gateway-specific settings (image, args, environment)
- Provides two volume mount configurations:
  - `/gateways` → gateway configuration files
  - `/supergraph` → supergraph schema and vendor configs

### Docker Manager (`crates/cli/src/docker.rs`)

- Uses `duct` for subprocess execution
- Operations: 
  - `compose_up_services`: Start specific subgraph services
  - `compose_down`: Stop all containers
  - `run`: Start gateway container
  - `stop`: Stop and remove container
- Gateway containers run with:
  - Network mode: host
  - Volume mounts for configs and supergraph
  - Configurable environment variables and arguments

### Resource Monitor (`src/resources.rs`)

- Uses `bollard` Docker API client
- Collects CPU and memory stats every second
- Runs in background during K6 execution
- Calculates statistics (min, max, avg, p50, p95, p99)
- Filters samples to K6 execution window

### K6 Runner (`src/k6.rs`)

- Executes K6 tests with JSON output
- Captures precise start/end timestamps
- Parses K6 metrics from JSON output
- Coordinates with resource monitor

### Report Generator (`src/report.rs`)

- Generates JSON reports with benchmark results
- Combines K6 metrics with resource statistics
- Structures output for analysis and comparison

## Gateway Configuration Format

`gateways/config.toml`:

```toml
[grafbase]
label = "Grafbase Gateway"
image = "ghcr.io/grafbase/gateway:latest"
args = ["--config", "/gateways/grafbase/grafbase.toml", "-s", "/supergraph/schema.graphql"]
environment = { LOG_LEVEL = "info" }

[apollo-router]
label = "Apollo Router"
image = "ghcr.io/apollographql/router:latest"
args = ["-c", "/gateways/apollo-router/router.yml", "-s", "/supergraph/schema.graphql"]
```

## CLI Usage

```bash
# Run benchmarks using default configuration
cargo run -- run

# Run benchmarks using specific configuration file
cargo run -- run my-config

# List available benchmark configurations
cargo run -- list

# List details of specific benchmark configuration
cargo run -- list default
```

## Dependencies

- `argh`: CLI argument parsing
- `bollard`: Docker API client for metrics
- `duct`: Process execution for docker commands
- `jiff`: Timestamp handling with serde support
- `tokio`: Async runtime
- `reqwest`: HTTP client for health checks
- `statrs`: Statistical calculations
- `tracing`: Structured logging

## Important Notes for Future Work

### Required Files Structure

1. **Benchmark Configuration** (`benchmarks/[name].toml`): Defines scenario × gateway matrix
2. **Scenario** (`scenarios/[name]/`): Contains K6 script and test configuration
3. **Supergraph** (`supergraphs/[name]/`): Contains federation schema and vendor configs
4. **Gateway Configs** (`gateways/[name]/`): Gateway-specific configuration files
5. **Main Compose** (`compose.yml`): All subgraph service definitions

### Assumptions

- Gateway runs on port 4000 (hardcoded in health check)
- K6 outputs JSON to `results.json` in scenario directory
- Subgraphs use docker compose with `--wait` flag support
- Gateway containers use `/supergraph` as schema mount path

### Error Handling

- Graceful cleanup on failure (stops containers)
- 30-second timeout for gateway health check
- Automatic container removal after stopping

### Metrics Collection

- Samples collected every 1 second
- CPU percentage calculated from Docker stats
- Memory in bytes converted to MB for output
- Only analyzes samples within K6 execution window

### Output Format

JSON structure with:
- Benchmark and gateway names
- Execution timestamps and duration
- K6 metrics (iterations, duration stats, data transfer)
- Gateway resource statistics (CPU%, memory MB)

### Testing Commands

```bash
# Check for lints
cargo clippy -p cli

# Run with debug logging
RUST_LOG=debug cargo run -p cli -- run

# List available configurations
cargo run -p cli -- list

# Run specific benchmark configuration
cargo run -p cli -- run my-benchmark
```
