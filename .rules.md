# Coding guidelines

Code MUST be clear and concise and the same applies to comments. Clear code does not need comments explaining what it does. But you may explain _why_ you're solving a problem in a particular way.

Prefer re-using existing files. For new components, avoid creating many small files.

## Rust

### Error handling

In production code, always handle `Result` and `Option` with care when you need to retrieve the value:

```rust
# propagate errors with `?`
let result = f()?;

# custom handling
match f() {
    Ok(v) => g(v),
    Err(e) => h(e)
}
```

If it can never be `None` or an error, use `expect("reason")` or `unreachable!("reason")` in a match case rather than `.unwrap()`.

### String formatting

Prefer Use modern string formatting `format!("{value}")` rather than `format!("{}", value)`.

### Logging

Do not abuse of the `info` level with logging such as `tracing::info!`, use `debug` instead.

### Lints

You may check for errors with:

```sh
cargo clippy -p <package>
```

### Tests

In test code, you may use `.unwrap()`.

Avoid test names like `test_` and prefer using terms like "can", "should", "must" or "given" to define a property/behavior the code should uphold.

Use inline snapshots with `insta` crate rather than a chain of `assert_eq!`:

```rust
insta::assert_json_snapshot!(value, @r#""#);
// or
insta::assert_snapshot!(error, @r#""#);
```

If you do want to compare to variables, import the `assert_*` variants from the `pretty_asesertions` crate.

To generate the initial snapshots, you may run the tests with the `INSTA_FORCE_PASS=1` environment variable and use `cargo insta approve` to apply all the snapshots.

You can run tests with `nextest` as follows:

```sh
cargo nextest run -p <package>
# or for a particular set of tests
cargo nextest run -p <package> <test prefix or name>
```

## Markdown conventions

- Always use an empty line after a heading and before starting a list.

# Structure

## Project Overview

This is a GraphQL Federation Benchmarking System that orchestrates performance tests across different federation gateways. It runs K6 load tests while monitoring gateway resource usage (CPU/memory) and produces detailed JSON reports.

## Architecture

```
graphql-federation-benchmarks/
├── src/
│   ├── main.rs           # CLI entry point with tracing setup
│   ├── cli.rs            # Command definitions using argh
│   ├── orchestrator.rs   # Core orchestration logic for running benchmarks
│   ├── docker.rs         # Docker operations (compose, containers)
│   ├── metrics.rs        # Resource monitoring via Docker stats API
│   ├── k6.rs             # K6 test execution and result parsing
│   └── results.rs        # Output data structures
│
├── benchmarks/           # Benchmark definitions
│   └── [benchmark-name]/
│       ├── compose.yml   # Docker compose for subgraphs
│       ├── k6.js         # K6 load test script
│       └── gateways/
│           ├── config.toml          # Gateway configurations
│           └── [gateway-name]/      # Gateway-specific files
│               └── [config files]
│
└── plan.md              # Original implementation plan
```

## Key Components

### Orchestrator (`src/orchestrator.rs`)

- Discovers benchmarks from `benchmarks/` directory
- Loads gateway configurations from TOML files
- Manages benchmark execution workflow:
  1. Start subgraphs via docker compose
  2. Start gateway container with host networking
  3. Health check gateway
  4. Run K6 test with metrics collection
  5. Cleanup resources
- Handles filtering by benchmark/gateway names

### Docker Manager (`src/docker.rs`)

- Uses `duct` for subprocess execution
- Operations: `compose_up`, `compose_down`, `run_gateway`, `stop_container`
- Gateway containers run with:
  - Network mode: host
  - Volume mount: `gateways/[name]/` → `/data`
  - Configurable environment variables and arguments

### Metrics Collector (`src/metrics.rs`)

- Uses `bollard` Docker API client
- Collects CPU and memory stats every second
- Runs in background during K6 execution
- Calculates statistics (min, max, avg, p50, p95, p99)
- Filters samples to K6 execution window

### K6 Runner (`src/k6.rs`)

- Executes K6 tests with JSON output
- Captures precise start/end timestamps
- Parses K6 metrics from JSON output
- Coordinates with metrics collector

## Gateway Configuration Format

`benchmarks/[name]/gateways/config.toml`:

```toml
[[gateways.grafbase]]
image = "ghcr.io/grafbase/gateway:latest"
arguments = ["--schema", "/data/schema.graphql"]
environment = { LOG_LEVEL = "info" }

[[gateways.apollo_router]]
image = "ghcr.io/apollographql/router:latest"
arguments = ["--supergraph", "/data/supergraph.graphql"]
```

## CLI Usage

```bash
# Run specific benchmark with specific gateway
cargo run -- run --benchmark many-possible-query-plans --gateway grafbase

# Run all benchmarks with all gateways
cargo run -- run

# List available benchmarks and gateways
cargo run -- list
```

## Dependencies

- `argh`: CLI argument parsing
- `bollard`: Docker API client for metrics
- `duct`: Process execution for docker commands
- `jiff`: Timestamp handling with serde support
- `tokio`: Async runtime
- `reqwest`: HTTP client for health checks
- `statrs`: Statistical calculations
- `tracing`: Structured logging

## Important Notes for Future Work

### Required Files for Each Benchmark

1. `compose.yml`: Docker compose file for subgraphs
2. `k6.js`: K6 test script
3. `gateways/config.toml`: Gateway configurations
4. `gateways/[gateway-name]/`: Directory with gateway-specific config files

### Assumptions

- Gateway runs on port 4000 (hardcoded in health check)
- K6 outputs JSON to `results.json` in benchmark directory
- Subgraphs use docker compose with `--wait` flag support
- Gateway containers use `/data` as config mount path

### Error Handling

- Graceful cleanup on failure (stops containers)
- 30-second timeout for gateway health check
- Automatic container removal after stopping

### Metrics Collection

- Samples collected every 1 second
- CPU percentage calculated from Docker stats
- Memory in bytes converted to MB for output
- Only analyzes samples within K6 execution window

### Output Format

JSON structure with:
- Benchmark and gateway names
- Execution timestamps and duration
- K6 metrics (iterations, duration stats, data transfer)
- Gateway resource statistics (CPU%, memory MB)

### Testing Commands

```bash
# Check for lints
cargo clippy

# Run with debug logging
RUST_LOG=debug cargo run -- run

# Run specific test
cargo run -- run -b many-possible-query-plans -g grafbase
```
